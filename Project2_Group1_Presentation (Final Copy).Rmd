---
title: "CMDA 4654 Project 2"
author: "Julia Brady, Priya Bhat, Mason Colt, Matt Nissen, Dylan Fair, Jamal Mani"
date: "2025-11-22"
output:
  slidy_presentation:
    highlight: kate
  ioslides_presentation:
    highlight: kate
  beamer_presentation:
    fonttheme: professionalfonts
    highlight: kate
    theme: Rochester
classoption: aspectratio=169
fontsize: 9pt
header-includes:
- \definecolor{VTmaroon}{HTML}{861F41}
- \usecolortheme[named=VTmaroon]{structure}
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
library(plotly)
library(lars)
library(tibble)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(echo = FALSE)

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  paste0("\n \\", "footnotesize","\n\n", x, "\n\n \\normalsize")
})
```

## What is Least Angle Regression?

- A regression algorithm for high-dimensional data  
- Builds the model incrementally like forward selection  
- But **less greedy** and more **statistically efficient**  
- Computes the entire **LASSO path** with a small modification  
- Produces smooth, piecewise-linear coefficient trajectories  



## Why Do We Need LARS?

**Forward Selection Problems:**

- Commits too strongly to the first chosen variable  
- Struggles with correlated predictors  
- Greedy -> unstable -> can miss important variables

**LARS Fixes This:**

- Takes controlled, geometric steps  
- Adjusts direction whenever correlations change  
- Never "overcommits" too early  
- Fair to correlated predictors



## Key Idea

> LARS moves in the direction that forms **equal angles** with all predictors most correlated with the residual.

- A balanced "least angle" update  
- A clear sequence of variable entry  
- A smooth coefficient path  



## Setup and Notation

We model:

\[
\mu = X\beta
\]

At any step:

- Residual:  
\[
r = y - X\beta
\]

- Correlations with residual:
\[
c_j = x_j^\top r
\]

- Active set:
\[
A = \{ j : |c_j| = \max_k |c_k| \}
\]



## The Equiangular Direction

To update the model, LARS finds a vector \(u_A\) such that:

- Every active predictor makes the **same angle** with \(u_A\)  
- Each active predictor has the **same correlation** with the update

Mathematically:

\[
u_A = X_A w_A, \quad 
w_A = \frac{G_A^{-1} 1_A}{\sqrt{1_A^\top G_A^{-1}1_A}}
\]

- \(X_A\) is the matrix of active predictors  
- \(G_A = X_A^\top X_A\)  
- \(1_A\) is a vector of ones  

This ensures a **balanced movement** among active predictors.



## Updating the Model

The fitted values update via:

\[
\mu \leftarrow \mu + \gamma\, u_A
\]

Where the step size \(\gamma\) is the **largest value** such that:

- All active predictors remain tied  
- A new predictor reaches the same correlation level

Thus the active set expands exactly when it should.

## Algorithm Steps
Initial Equation: $y = \beta_{0}$
Final Equation:  $y = \beta_{0} + \beta_{1}x_1 + \beta_{2}x_2 + ... + \beta_{n}x_n$

1. Take the correlation of residuals with every predictor variable and find the maximum.
2. Add the highest-correlation predictor $x_j$ to the equation with coefficient $\gamma$: $r = y- \beta_0 - \gamma_j x_j$, $y = \beta_0 + \gamma_j x_j$
3. Increase $\gamma$ in the direction of its correlation with y (positive or negative), taking residuals along the way, until $Cor(x_j, r) = Cor(x_k, r)$ for some other predictor $x_k$.
4. Continue moving along $(x_j, x_k)$ by increasing $(\gamma_j, \gamma_k)$ in their least squares direction, taking residuals along the way, until some predictor $x_m$ has as much correlation as the residual.
5. Repeat this until all predictors are included in the model.

## Algorithm

1.  Take the correlation of residuals with every predictor variable and find the maximum.

At the beginning of the algorithm, we set our intercept $\beta_0$ equal to the average of our y-vector, denoted by $\bar y$. Residuals, then, are given by $r = y - \beta_0 = y - \bar y$

We can denote the correlation of residuals with each variable as $Cor(r, \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}) = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix}$

Let $max \{ {\begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix}} \}=c_{max}$.

We select the predictor $x_j$ corresponding to $c_{max}$ as our first active predictor.

## Algorithm

2.  Add the highest-correlation predictor $x_j$ to the equation with coefficient $\gamma$:

$r = y- \beta_0 - \gamma_j x_j$

$\gamma_j$ is a temporary step size along predictor $x_j$. We will compute its value and assign it to $\beta_j$.

## Algorithm

3.  Increase $\gamma$ in the direction of its correlation with y (positive or negative), taking residuals along the way, until $Cor(x_j, r) = Cor(x_k, r)$ for some other predictor $x_k$.

The direction of correlation is given by $u$, a unit vector equal to $x_j$. The residual vector, constantly being updated as $\gamma$ increases, is given by $r(\gamma)$.

$r(\gamma) = r_0 - \gamma u$, where $r_0 = y - \bar y$.

To find $\gamma_j$, consider all predictors not added to the model yet. Solve

$C = Cor(x_j, r(\gamma))=Cor(x_i, r(\gamma))$. This will give us $(n-1)$ values of $\gamma$ for $n$ predictive variables.

The minimum $\gamma$ value is our $\beta_j$. The corresponding predictor is the next active predictor, which we will add in the next step and denote $x_k$.

Now, we have $y = \beta_0 + \beta_j x_j$.

## Algorithm

4.  Continue moving along $(x_j, x_k)$ in their least squares direction, taking residuals along the way, until some predictor $x_m$ has as much correlation as the residual.

Because we now have 2 predictors, our direction vector $u$ will be updated according to both. Also, we will move $\gamma_k$ to update our fitted values and residuals.

Then, for every predictor we're not already using, we find the minimum $\gamma$ value again. It will update each of our coefficients and the corresponding predictor $x_m$ will be added to the model next.

Now, we have our updated $\beta_k$, and our equation becomes $y = \beta_0 + \beta_j x_j + \beta_k x_k$.

## Algorithm

5.  Continue moving along $(x_j, x_k, x_m)$ in their least squares direction, taking residuals along the way, until some predictor $x_p$ has as much correlation as the residual vector.

Once again, our direction vector $u$ is updated according to all the predictors, and we will move $\gamma_m$ to update both our fitted values and residuals.

We find the minimum $\gamma$ value among all non-active predictors, update our coefficients, and we will add $x_n$ to the next iteration of the model.

6.  We will repeat this until all predictors are included in the model.

This movement is **piecewise-linear** and **geometrically optimal**.


## Geometric Picture
```{r echo = FALSE}
# Clean conceptual LARS path for 3 predictors:
#   Segment 1: Predictor 1 -> movement in 1D (beta1)
#   Segment 2: Predictors 1 & 2 -> movement in 2D plane (beta1, beta2)
#   Segment 3: Predictors 1, 2 & 3 -> movement in 3D (beta1, beta2, beta3)

lars_path <- data.frame(
  beta1 = c(0, 0.6, 1.0, 1.55),
  beta2 = c(0, 0.0, 0.4, 1.10),
  beta3 = c(0, 0.0, 0.0, 0.90),
  label = c(
    "Origin (0 predictors)",
    "Predictor 1 active (1D)",
    "Predictor 2 active (2D)",
    "Predictor 3 active -> OLS direction (3D)"
  ),
  segment = c("1D", "1D", "2D", "3D")
)

# Colors for subspaces
seg_colors <- c("1D" = "lightblue", "2D" = "steelblue", "3D" = "blue")

# Build 3D interactive visualization

plot_ly() %>%
  
  # 1D segment (Predictor 1 only)
  add_trace(
    data = lars_path[1:2, ],
    x = ~beta1, y = ~beta2, z = ~beta3,
    type = "scatter3d", mode = "lines+markers+text",
    line = list(width = 6, color = seg_colors["1D"]),
    marker = list(size = 4, color = seg_colors["1D"]),
    name = "1D movement"
  ) %>%
  add_trace(  # label only the END of segment
    data = lars_path[2, ],
    x = ~beta1, y = ~beta2, z = ~beta3,
    type = "scatter3d", mode = "text",
    text = ~label, textposition = "top center",
    showlegend = FALSE
  ) %>%
  
  # 2D segment (Predictors 1 & 2)
  add_trace(
    data = lars_path[2:3, ],
    x = ~beta1, y = ~beta2, z = ~beta3,
    type = "scatter3d", mode = "lines+markers+text",
    line = list(width = 6, color = seg_colors["2D"]),
    marker = list(size = 4, color = seg_colors["2D"]),
    name = "2D movement"
  ) %>%
  add_trace(  # label only the END of segment
    data = lars_path[3, ],
    x = ~beta1, y = ~beta2, z = ~beta3,
    type = "scatter3d", mode = "text",
    text = ~label, textposition = "top center",
    showlegend = FALSE
  ) %>%
  
  # 3D segment (Predictors 1, 2 & 3)
  add_trace(
    data = lars_path[3:4, ],
    x = ~beta1, y = ~beta2, z = ~beta3,
    type = "scatter3d", mode = "lines+markers+text",
    line = list(width = 6, color = seg_colors["3D"]),
    marker = list(size = 4, color = seg_colors["3D"]),
    name = "3D movement"
  ) %>%   
  add_trace(  # final label
    data = lars_path[4, ],
    x = ~beta1, y = ~beta2, z = ~beta3,
    type = "scatter3d", mode = "text",
    text = ~label, textposition = "top center",
    showlegend = FALSE
  ) %>%
  
  
  layout(
    title = "Clean 3D Visualization of the LARS Path (3 Predictors)",
    legend = list(orientation = "h"),
    scene = list(
      xaxis = list(title = "β₁"),
      yaxis = list(title = "β₂"),
      zaxis = list(title = "β₃")
    )
  )
```


## When LARS is Appropriate

- **High-dimensional data with many predictors**: Excellent for datasets where the number of predictors exceeds the number of observations ($p > n$).

- **Variable selection is a priority**: Provides a clear and complete solution path showing the sequence and importance of features entering the model.

- **Computational efficiency matters**: Computes the full regularization path with similar speed to a single least squares fit, $O$($k^3+pk^2$).

- **Collinearity among predictors**: Effectively manages correlated predictors by moving them together, helping to identify meaningful feature groups.


## Real-World Examples: LARS is Appropriate

- **Genomics**: Gene expression analysis with thousands of genes but few samples ($p >> n$).

- **Financial modeling**: Portfolio optimization with hundreds of assets and economic indicators.

- **Medical imaging**: Disease prediction from high-dimensional MRI/CT features with limited patient data.

- **Marketing analytics**: Customer behavior modeling with many potential predictors (demographics, interactions, preferences).


## When LARS is NOT Appropriate

- **Non-linear relationships**:  Assumes linearity; unsuitable for data with non-linear patterns between features and the response.

- **Categorical variables dominate**: Best suited for continuous predictors; extensive categorical data may not fully leverage LARS's benefits.

- **Outliers are present**: Highly susceptible to outliers, which can significantly distort the regression path (consider alternatives).

- **Temporal Dependencies**: Fails to capture inherent trends, seasonality, or autocorrelation present in time-series data.

- **Very large sample size, few features**: Standard linear regression is often simpler and equally effective when observations heavily outweigh the number of features.


## Real-World Examples: LARS is NOT Appropriate

- **Image recognition**: Deep non-linear relationships between pixels and object classes.

- **Stock price prediction**: Time-series with autocorrelation, trends, and temporal dependencies.
- **Survey data with outliers**: Data quality issues where extreme responses distort the model.
- **Simple A/B testing**: Few variables with large sample sizes where standard regression suffices.


```{r echo = FALSE}
#install.packages("lars")
#library(lars)
data(iris)

```


## LARS using iris Dataset

LARS requires us to have:

- a numeric vector for the response (y)
- a matrix of predictor variables (x)

```{r echo = TRUE}
y <- iris$Sepal.Length
x <- as.matrix(iris[, c("Sepal.Width", "Petal.Length", "Petal.Width")])

```


```{r echo = FALSE}
lars_model <- lars(x, y, type = "lar")
```

## Model Summary
```{r}
lars_model
```


## iris LARS Plot

We can see that **Petal.Length** has the strongest correlation with Sepal.Length. **Sepal.Width** has the 2nd strongest correlation, and **Petal.Width** is the least correlated.
```{r}
plot(lars_model)
legend("topleft", legend = colnames(x), col = 1:3, lty = 1)
```

```{r echo = FALSE}
data(mtcars)

head(mtcars)


```


```{r echo = FALSE}
y_val <- mtcars$mpg
x_val <- as.matrix(mtcars[, -1])

#lars model
lars_model_mt <- lars(x_val, y_val, type = "lar")

```

## LARS using mtcars Dataset

In this model, we are looking to predict fuel efficiency:

- Miles per Gallon (mpg) as our response variable (y)
- Matrix of all predictor variables (x)
```{r}
head(mtcars)
```

## Model Summary

```{r}
summary(lars_model_mt)
```


## mtcars LARS Plot

Here we see that **wt** and **cyl** enter the model first, meaning they are more strongly correlated to **mpg** than other variables such as **disp** and **qsec**. 
```{r}
plot(lars_model_mt)
legend("bottomleft", legend = colnames(x_val), col = 1:10, lty = 1, pch = 16, cex = 0.70)
```

## Key Findings

- Through these two examples, we can see that LARS makes it very easy to identify which variables have the most influence over our response variable.

- It shows us how important each predictor is, while also showing us where each variable enters model.

- LARS also adds variables into the model in a balanced way without over fitting.


## Comparing LARS to Other Regression Methods

```{r lars_comparison_setup, include=FALSE}
library(tidyverse)
library(glmnet)
library(MASS)
library(lars)

set.seed(123)

mse_fun <- function(y_true, y_pred) {
  mean((y_true - y_pred)^2)
}

# ---------------- IRIS: Predict Sepal.Length ----------------

# Use only numeric predictors
iris_df <- iris %>%
  as_tibble() %>%
  dplyr::select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)

# Train / test split (75/25)
n_iris  <- nrow(iris_df)
train_i <- sample(seq_len(n_iris), size = floor(0.75 * n_iris))

iris_train <- iris_df[train_i, ]
iris_test  <- iris_df[-train_i, ]

# OLS
ols_iris <- lm(Sepal.Length ~ ., data = iris_train)
pred_ols_iris <- predict(ols_iris, newdata = iris_test)
mse_ols_iris  <- mse_fun(iris_test$Sepal.Length, pred_ols_iris)

# Stepwise (both directions)
step_iris <- stepAIC(ols_iris, direction = "both", trace = FALSE)
pred_step_iris <- predict(step_iris, newdata = iris_test)
mse_step_iris  <- mse_fun(iris_test$Sepal.Length, pred_step_iris)

# Lasso (glmnet)
x_train_iris <- model.matrix(Sepal.Length ~ ., iris_train)[, -1]
y_train_iris <- iris_train$Sepal.Length
x_test_iris  <- model.matrix(Sepal.Length ~ ., iris_test)[, -1]
y_test_iris  <- iris_test$Sepal.Length

cv_lasso_iris <- cv.glmnet(
  x = x_train_iris,
  y = y_train_iris,
  alpha = 1,
  family = "gaussian"
)
best_lambda_iris <- cv_lasso_iris$lambda.min

lasso_iris <- glmnet(
  x = x_train_iris,
  y = y_train_iris,
  alpha = 1,
  family = "gaussian",
  lambda = best_lambda_iris
)

pred_lasso_iris <- predict(lasso_iris, newx = x_test_iris)
mse_lasso_iris  <- mse_fun(y_test_iris, pred_lasso_iris)

# LARS (Lasso path)
lars_iris <- lars(x_train_iris, y_train_iris, type = "lasso")
best_step_iris <- which.min(lars_iris$Cp)

pred_lars_iris <- predict(
  lars_iris,
  newx = x_test_iris,
  s = best_step_iris,
  mode = "step"
)$fit
mse_lars_iris <- mse_fun(y_test_iris, pred_lars_iris)

compare_iris <- tibble(
  Model    = c("OLS", "Stepwise", "Lasso", "LARS"),
  Test_MSE = c(mse_ols_iris, mse_step_iris, mse_lasso_iris, mse_lars_iris)
)

compare_iris

# ---------------- MTCARS: Predict mpg ----------------


mtcars_df <- mtcars %>%
  as_tibble()

# Train / test split (75/25)
n_mtc  <- nrow(mtcars_df)
train_m <- sample(seq_len(n_mtc), size = floor(0.75 * n_mtc))

mtcars_train <- mtcars_df[train_m, ]
mtcars_test  <- mtcars_df[-train_m, ]

# OLS
ols_mtcars <- lm(mpg ~ ., data = mtcars_train)
pred_ols_mtcars <- predict(ols_mtcars, newdata = mtcars_test)
mse_ols_mtcars  <- mse_fun(mtcars_test$mpg, pred_ols_mtcars)

# Stepwise
step_mtcars <- stepAIC(ols_mtcars, direction = "both", trace = FALSE)
pred_step_mtcars <- predict(step_mtcars, newdata = mtcars_test)
mse_step_mtcars  <- mse_fun(mtcars_test$mpg, pred_step_mtcars)

# Lasso (glmnet)
x_train_mtcars <- model.matrix(mpg ~ ., mtcars_train)[, -1]
y_train_mtcars <- mtcars_train$mpg
x_test_mtcars  <- model.matrix(mpg ~ ., mtcars_test)[, -1]
y_test_mtcars  <- mtcars_test$mpg

cv_lasso_mtcars <- cv.glmnet(
  x = x_train_mtcars,
  y = y_train_mtcars,
  alpha = 1,
  family = "gaussian"
)
best_lambda_mtcars <- cv_lasso_mtcars$lambda.min

lasso_mtcars <- glmnet(
  x = x_train_mtcars,
  y = y_train_mtcars,
  alpha = 1,
  family = "gaussian",
  lambda = best_lambda_mtcars
)

pred_lasso_mtcars <- predict(lasso_mtcars, newx = x_test_mtcars)
mse_lasso_mtcars  <- mse_fun(y_test_mtcars, pred_lasso_mtcars)

# LARS
lars_mtcars <- lars(x_train_mtcars, y_train_mtcars, type = "lasso")
best_step_mtcars <- which.min(lars_mtcars$Cp)

pred_lars_mtcars <- predict(
  lars_mtcars,
  newx = x_test_mtcars,
  s = best_step_mtcars,
  mode = "step"
)$fit
mse_lars_mtcars <- mse_fun(y_test_mtcars, pred_lars_mtcars)

compare_mtcars <- tibble(
  Model    = c("OLS", "Stepwise", "Lasso", "LARS"),
  Test_MSE = c(mse_ols_mtcars, mse_step_mtcars,
               mse_lasso_mtcars, mse_lars_mtcars)
)
```

## Iris: LARS vs Other Regression Methods

We predict **Sepal.Length** from the three other numeric iris variables and compare four models:

- OLS Test MSE: `r round(mse_ols_iris, 3)`
- Stepwise Test MSE: `r round(mse_step_iris, 3)`
- Lasso Test MSE: `r round(mse_lasso_iris, 3)`
- LARS Test MSE: `r round(mse_lars_iris, 3)`

**Interpretation:**

- All four models have very similar test error because iris is small and low-dimensional.
- Lasso achieves the lowest MSE due to coefficient shrinkage (slightly reducing variance).
- LARS nearly matches Lasso’s performance because its solution path closely approximates the Lasso path.
- Stepwise and OLS perform similarly since most predictors are genuinely informative.

```{r iris_table, echo=FALSE}
knitr::kable(compare_iris, digits = 3)
```

## Iris: Lasso Cross-Validation Curve

Lasso selects its penalty parameter λ using cross-validation.

- Small λ → more flexible model
- Large λ → stronger shrinkage
- The dashed vertical line marks the λ minimizing CV error

```{r iris_lasso_cv, echo=FALSE, fig.width=6, fig.height=4}
oldpar <- par(no.readonly = TRUE)

par(mar = c(6, 4, 3, 1), cex.lab = 0.9, cex.axis = 0.9)

x   <- -log(cv_lasso_iris$lambda)
y   <- cv_lasso_iris$cvm
se  <- cv_lasso_iris$cvsd
x_min <- -log(cv_lasso_iris$lambda.min)

# Base plot
plot(
  x, y,
  type = "n",
  xlab = expression(-log(lambda)),
  ylab = "Mean-Squared Error",
  main = "Iris: Lasso CV Curve"
)

arrows(
  x0 = x, y0 = y - se,
  x1 = x, y1 = y + se,
  angle = 90, code = 3, length = 0.02,
  col = "lightgray"
)

lines(x, y, col = "gray40")
points(x, y, pch = 16, col = "red", cex = 0.6)

abline(v = x_min, lty = 2)

par(oldpar)

```



## Iris: LARS Coefficient Paths

The LARS coefficient path shows:

- The order predictors enter the model
- How quickly coefficients grow
- The piecewise-linear nature of the LARS solution

```{r iris_lars_path, echo=FALSE, fig.width=6, fig.height=4}
oldpar <- par(no.readonly = TRUE)

par(mar = c(6, 4, 3, 1),
    cex.lab  = 0.9,
    cex.axis = 0.9,
    cex.main = 1)


beta_mat <- lars_iris$beta
steps    <- seq_len(nrow(beta_mat))

cols <- c("red", "black", "darkgreen")

matplot(
  steps, beta_mat,
  type = "l",
  lty  = 1,
  col  = cols,
  xlab = "Step in LARS path",
  ylab = "Standardized Coefficients",
  main = "Iris: LARS Coefficient Paths"
)

abline(h = 0, lty = 3)

legend(
  "topleft",
  legend = colnames(beta_mat),  
  col    = cols,
  lty    = 1,
  bty    = "n",
  cex    = 0.8
)

par(oldpar)


```


## mtcars: LARS vs Other Regression Methods

We predict **mpg** in the mtcars dataset using all other variables:

- OLS Test MSE: `r signif(mse_ols_mtcars, 3)`
- Stepwise Test MSE: `r signif(mse_step_mtcars, 3)`
- Lasso Test MSE: `r signif(mse_lasso_mtcars, 3)`
- LARS Test MSE: `r signif(mse_lars_mtcars, 3)`

**Interpretation:**

- mtcars is extremely small (32 rows), so each train/test split is noisy.
- All four models have almost identical MSE on this split.
- Lasso and LARS again behave very similarly in terms of prediction.
- With such a tiny dataset, shrinkage and selection provide limited gains over OLS, but they illustrate the methods’ behavior.

```{r mtcars_table, echo=FALSE}
knitr::kable(compare_mtcars, digits = 4)
```

## Takeaways from Baby Datasets

- On simple, low-dimensional datasets, all four models achieve very similar performance.
- Lasso and LARS consistently match or slightly beat OLS and Stepwise.
- LARS is especially appealing when:
  - the number of predictors is large,
  - predictors are correlated,
  - and we care about the full coefficient path for interpretation.
- These baby examples confirm that LARS behaves like Lasso in terms of predictive accuracy, while providing an efficient and interpretable solution path.

## Main Dataset

This project uses the Burke et al. (2022) global urban soil black carbon dataset, obtained from the Knowledge Network for Biocomplexity (KNB) at:
https://knb.ecoinformatics.org/view/urn:uuid:1651eeb1-e050-4c78-8410-ec2389ca2363

The dataset pulls together measurements of black carbon in urban soils from cities around the world. Each row includes details like latitude/longitude, elevation, precipitation, soil temperature at different depths, land-cover type, population info, and notes from the original studies. The main sheet (“Urban Black Carbon”) contains 600+ observations and about 65 variables, giving us a wide mix of environmental and geographic predictors.

Because many of these variables move together (climate, location, soil traits, etc.), the dataset naturally has clusters of correlated features, which makes it a solid fit for demonstrating Least Angle Regression (LARS).



## Data Dictionary

```{r load-data, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
library(readxl)
library(tidyverse)
library(janitor)
library(skimr)
library(kableExtra)

soil_raw <- read_excel("Burke_soilBC_2022.xlsx",
                       sheet = "Urban Black Carbon")
```


```{r}
#clean col names to snake_case 
soil <- soil_raw %>%
  clean_names()


soil <- soil %>%
  mutate(across(
      c(urban_land_use, urban_land_cover_2),
      ~ na_if(.x, "-999")
    )
  )

missing_tbl <- soil %>%
  summarize(across(everything(), ~ mean(is.na(.)) * 100)) %>%
  pivot_longer(everything(),
               names_to = "variable",
               values_to = "pct_missing") %>%
  arrange(desc(pct_missing))
```

```{r echo=F}
vars_keep <- missing_tbl %>%
  filter(pct_missing < 90) %>%
  pull(variable)

soil_reduced <- soil %>% dplyr::select(dplyr::all_of(vars_keep))
```


```{r include = FALSE}
response_var <- "bc_mg_g"

predictors <- c(
  "latitude_from_ge",
  "longitude_from_ge",
  "combined_elevation_masl",
  "combined_precipitation_mm_yr",
  "mean_annual_soil_temp_0_5_cm_c_1979_2013",
  "mean_annual_soil_temp_5_15_cm_c_1979_2013",
  "depth_cm",
  "depth_range_cm"
)

soil_model <- soil_reduced %>%
  dplyr::select(dplyr::all_of(c(response_var, predictors)))

#complete cases
soil_model_clean <- soil_model %>% 
  drop_na()

n_before <- nrow(soil_model)
n_after  <- nrow(soil_model_clean)

#fix depth range to numeric
soil_model_clean <- soil_model_clean %>%
  mutate(
    depth_range_cm = depth_range_cm %>%
      str_split("-", simplify = TRUE) %>%
      apply(1, \(x) mean(as.numeric(x)))
  )

#standardize predictors
soil_lars <- soil_model_clean %>%
  mutate(across(all_of(predictors), scale))
```



```{r echo=FALSE}
library(kableExtra)

data_dict <- tibble(
  variable = c(response_var, predictors),
  description = c(
    "Black carbon concentration (mg/g)",
    "Latitude of sample location",
    "Longitude of sample location",
    "Elevation (masl)",
    "Annual precipitation (mm/year)",
    "Annual mean soil temperature 0–5 cm (°C, 1979–2013)",
    "Annual mean soil temperature 5–15 cm (°C, 1979–2013)",
    "Sampling depth (cm)",
    "Depth range of sample (cm)"
  ),
  type = c("numeric", rep("numeric", length(predictors)))
)

data_dict %>%
  kable(
    format = "latex",
     booktabs = TRUE,
    caption = "Data Dictionary for Modeling Variables",
    col.names = c("Variable", "Description", "Type")
  ) %>%
  kable_styling(
    latex_options = c("hold_position")
  )
```
We removed variables with 90+% missing values to avoid unstable predictors and ensure consistent sample size across all variables. This threshold preserved essential environmental predictors while excluding sparse fields that contained too little information to contribute to modeling.




## BC vs Depth
```{r}
## bc vs depth
ggplot(soil_lars, aes(x = depth_cm, y = bc_mg_g)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, color = "steelblue") +
  labs(
    title = "BC Concentration vs Sampling Depth",
    x = "Depth (cm)",
    y = "BC (mg/g)"
  ) +
  theme_minimal()
```



## Takeaways

- High BC values occur almost entirely near the surface (0–1 cm), reflecting strong urban deposition.

- BC drops sharply with depth, flattening to low levels beyond ~4 cm.

- Variance is very large at shallow depths, but nearly zero deeper in the profile.

- Pattern indicates a nonlinear depth–BC relationship and potential heteroskedasticity.

- Supports LARS: depth correlates with other environmental factors, and regularization helps stabilize selection in the presence of such gradients.



## Predictor Correlation Heatmap 

```{r}
# correlation matrix among standardized predictors
corr_mat <- soil_lars %>%
  dplyr::select(dplyr::all_of(predictors)) %>%
  cor()

corr_df <- as_tibble(
  expand.grid(
    var1 = rownames(corr_mat),
    var2 = colnames(corr_mat)
  )
) %>%
  mutate(correlation = as.vector(corr_mat))

ggplot(corr_df, aes(x = var1, y = var2, fill = correlation)) +
  geom_tile() +
  scale_fill_gradient2(limits = c(-1, 1)) +
  coord_fixed() +
  labs(
    title = "Correlation Among Predictors",
    x = NULL,
    y = NULL,
    fill = "r"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```



## Takeaways

- Strong spatial correlation between latitude and longitude.

- The two soil temperature variables are highly correlated.

- Elevation and precipitation follow the same environmental gradient.

- Overall: clear multicollinearity clusters, meaning several predictors share similar information. This supports using LARS.



## LARS on Large Data
```{r echo = FALSE}
set.seed(123)

## Response + predictors
y_full <- soil_lars$bc_mg_g
X_full <- soil_lars %>% dplyr::select(-bc_mg_g)

n <- nrow(X_full)
train_idx <- sample(n, size = floor(0.75 * n))

## Train/test split
## 75/25 split
y_train <- y_full[train_idx]
y_test <- y_full[-train_idx]

X_train_raw <- X_full[train_idx, ]
X_test_raw <- X_full[-train_idx, ]

## Standardize using training statistics only
means <- apply(X_train_raw, 2, mean)
sds <- apply(X_train_raw, 2, sd)

X_train <- scale(X_train_raw, center = means, scale = sds)
X_test <- scale(X_test_raw, center = means, scale = sds)

X_train <- as.matrix(X_train)
X_test <- as.matrix(X_test)

## Fit LARS (lasso path)
lars_soil <- lars(X_train, y_train, type = "lasso")

## Choose optimal step using Mallow's Cp
# The step with the min Cp balances bias and variance
best_step <- which.min(lars_soil$Cp)

## Test predictions
soil_pred_test <- predict(lars_soil, newx = X_test, s = best_step, mode = "step")$fit

mse_lars_soil <- mean((soil_pred_test - y_test)^2)

# Create a small table for the slide showing LARS test performance
lars_results <- tibble(Model = "LARS (Cp-selected)", Test_MSE = mse_lars_soil)

```

```{r echo = FALSE}
lars_results %>% kable(format = "html", 
                       caption = "Test-Set Performance of the LARS Model", 
                       align = "cc") %>%
kable_styling(full_width = FALSE)
```

*Note:  On held-out soil samples, the model’s predictions differ from the observed black carbon values by about 12.4 mg/g on average*

This level of error is expected for this dataset, because black carbon concentrations are extremely variable near the soil surface.

## LARS Coefficient Path
The coefficient path illustrates:

- the order in which predictors enter the model

- the strength of their standardized effects

- and how the LARS moves piecewise-linearly through predictor space

```{r echo = FALSE}
# -------------------------------------------------------------------
# 17. Extract standardized coefficient paths across LARS steps.
# -------------------------------------------------------------------
beta_mat <- lars_soil$beta # rows = steps, cols = predictors
steps <- seq_len(nrow(beta_mat)) # step index along the path

# Color palette (length must match number of predictors)
cols <- RColorBrewer::brewer.pal(
  n   = ncol(beta_mat),
  name = "Dark2"
)

oldpar <- par(no.readonly = TRUE)

par(mar = c(5, 4, 3, 1), cex.lab = 0.9, cex.axis = 0.9, cex.main = 1)

# Plot standardized coefficients vs. step number
# This visualizes:
# - the order predictors enter the model
# - how large (or small) their effects become
# - where the Cp-selected step lies on the path
matplot(
  x = steps, y = beta_mat, type = "l", lty = 1, col  = cols,
  xlab = "Step in LARS path",
  ylab = "Standardized Coefficients",
  main = "Soil Black Carbon: LARS Coefficient Paths"
)

abline(h = 0, lty = 3, col = "gray70") # zero baseline
abline(v = best_step, lty = 2, col = "gray40") # Cp-selected step

legend("topleft", legend = colnames(beta_mat),
  col = cols, lty = 1, bty = "n", cex = 0.7)

par(oldpar)
```

## Interpretation

- Depth variables are the first to enter the model, with `depth_range_cm` entering immediately and taking on a large negative coefficient.
This confirms that black carbon decreases strongly with depth range, consistent with the earlier exploratory plots.


- Latitude and elevation enter early as well, both with strong positive standardized coefficients.
This indicates that locations at higher latitudes and higher elevations tend to have higher black carbon values, after adjusting for depth.


- Climate variables (precipitation and soil temperatures) enter in the middle of the path with moderate positive effects.
Their smoother, slower coefficient growth reflects their weaker marginal relationship after the major depth and geographic patterns are accounted for.


## Interpretation

- Longitude shows a small negative effect, entering later and remaining relatively flat.
This suggests that east-west location explains little additional variation once depth, latitude, and elevation are included.


- The dashed vertical line marks the Cp-selected step, which represents the best balance between model complexity and predictive accuracy.
Beyond this point, adding additional predictors primarily increases variance without improving model fit.


- Overall, LARS highlights a small set of dominant predictors (depth range, latitude, and elevation) while shrinking or delaying weaker, highly correlated variables, giving a clear picture of the main environmental drivers of black carbon.

## References
Lucero, Christian. “Model Selection.” CMDA-4654: Intermediate Machine Learning & Data Analytics, Lecture 15, Virginia Tech, Fall 2025.
https://en.wikipedia.org/wiki/Least-angle_regression
https://www.geeksforgeeks.org/machine-learning/least-angle-regression-lars/
https://tibshirani.su.domains/ftp/lars.pdf
https://projecteuclid.org/journals/annals-of-statistics/volume-32/issue-2/Least-angle-regression/10.1214/009053604000000067.full
https://link.springer.com/article/10.1007/s41237-024-00237-2
https://bookdown.org/yihui/rmarkdown-cookbook/figure-size.html
https://bookdown.org/yihui/rmarkdown/ioslides-presentation.html
https://statisticseasily.com/glossario/what-is-least-angle-regression-lars/

